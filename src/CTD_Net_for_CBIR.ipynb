{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2aKIury_ucD"
      },
      "outputs": [],
      "source": [
        "pip install numpy opencv-python scikit-learn tensorflow keras scipy\n",
        "!pip install PyWavelets\n",
        "pip install torch torchvision pandas pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#working in google colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths for input images and feature storage\n",
        "input_path = '/content/drive/MyDrive/wangdataset/Images/'  # Update this to your images folder\n",
        "output_path = '/content/drive/MyDrive/DeepLearning/'  # Update this to the folder for CSVs\n"
      ],
      "metadata": {
        "id": "hXFte3NHDGCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Function to compute color histogram in HSV space\n",
        "def compute_color_histogram(image, bins=(8, 2, 2)):\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    hist = cv2.calcHist([hsv_image], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])\n",
        "    hist = cv2.normalize(hist, hist).flatten()\n",
        "    return hist\n",
        "\n",
        "# Directory to save Color Histogram features\n",
        "color_histogram_csv = os.path.join(output_path, 'color_histogram_features.csv')\n",
        "\n",
        "# Process all images in the input directory\n",
        "image_files = [f for f in os.listdir(input_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "color_hist_features = []\n",
        "\n",
        "for image_name in image_files:\n",
        "    image_path = os.path.join(input_path, image_name)\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # Compute color histogram\n",
        "    hist_features = compute_color_histogram(image)\n",
        "\n",
        "    # Append to the list with image name\n",
        "    color_hist_features.append([image_name] + hist_features.tolist())\n",
        "\n",
        "# Save features to CSV\n",
        "color_hist_df = pd.DataFrame(color_hist_features)\n",
        "color_hist_df.to_csv(color_histogram_csv, index=False, header=False)\n",
        "print(f'Color Histogram features saved to {color_histogram_csv}')\n"
      ],
      "metadata": {
        "id": "nMac0b4wASMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute color moments (mean, std deviation)\n",
        "def compute_color_moments(image):\n",
        "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "    mean, std = [], []\n",
        "    for channel in cv2.split(hsv_image):\n",
        "        mean.append(np.mean(channel))\n",
        "        std.append(np.std(channel))\n",
        "    return mean + std\n",
        "\n",
        "# Directory to save Color Moments features\n",
        "color_moments_csv = os.path.join(output_path, 'color_moments_features.csv')\n",
        "\n",
        "# Process all images for color moments\n",
        "color_moments_features = []\n",
        "\n",
        "for image_name in image_files:\n",
        "    image_path = os.path.join(input_path, image_name)\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # Compute color moments\n",
        "    moments_features = compute_color_moments(image)\n",
        "\n",
        "    # Append to the list with image name\n",
        "    color_moments_features.append([image_name] + moments_features)\n",
        "\n",
        "# Save features to CSV\n",
        "color_moments_df = pd.DataFrame(color_moments_features)\n",
        "color_moments_df.to_csv(color_moments_csv, index=False, header=False)\n",
        "print(f'Color Moments features saved to {color_moments_csv}')\n"
      ],
      "metadata": {
        "id": "OUv7EEmxAXRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pywt\n",
        "\n",
        "# Function to compute Wavelet Transform\n",
        "def compute_wavelet_features(image, wavelet='db1', level=3):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    coeffs = pywt.wavedec2(gray_image, wavelet, level=level)\n",
        "    # Use mean and standard deviation of coefficients at each level\n",
        "    features = []\n",
        "    for coeff in coeffs:\n",
        "        if isinstance(coeff, tuple):  # for details coefficients (horizontal, vertical, diagonal)\n",
        "            for subband in coeff:\n",
        "                features.append(np.mean(subband))\n",
        "                features.append(np.std(subband))\n",
        "        else:  # for the approximation coefficients\n",
        "            features.append(np.mean(coeff))\n",
        "            features.append(np.std(coeff))\n",
        "    return features\n",
        "\n",
        "# Directory to save Wavelet Transform features\n",
        "wavelet_csv = os.path.join(output_path, 'wavelet_features.csv')\n",
        "\n",
        "# Process all images for wavelet features\n",
        "wavelet_features = []\n",
        "\n",
        "for image_name in image_files:\n",
        "    image_path = os.path.join(input_path, image_name)\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # Compute wavelet features\n",
        "    wavelet_features_list = compute_wavelet_features(image)\n",
        "\n",
        "    # Append to the list with image name\n",
        "    wavelet_features.append([image_name] + wavelet_features_list)\n",
        "\n",
        "# Save features to CSV\n",
        "wavelet_df = pd.DataFrame(wavelet_features)\n",
        "wavelet_df.to_csv(wavelet_csv, index=False, header=False)\n",
        "print(f'Wavelet features saved to {wavelet_csv}')\n"
      ],
      "metadata": {
        "id": "ldW27x1HAa9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.feature import local_binary_pattern\n",
        "\n",
        "# Function to compute LBP\n",
        "def compute_lbp_features(image, radius=2, n_points=16):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    lbp = local_binary_pattern(gray_image, n_points, radius, method=\"uniform\")\n",
        "    # Create histogram of LBP\n",
        "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "    # Normalize the histogram\n",
        "    hist = hist.astype(\"float\")\n",
        "    hist /= (hist.sum() + 1e-6)\n",
        "    return hist\n",
        "\n",
        "# Directory to save LBP features\n",
        "lbp_csv = os.path.join(output_path, 'lbp_features.csv')\n",
        "\n",
        "# Process all images for LBP features\n",
        "lbp_features = []\n",
        "\n",
        "for image_name in image_files:\n",
        "    image_path = os.path.join(input_path, image_name)\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # Compute LBP features\n",
        "    lbp_features_list = compute_lbp_features(image)\n",
        "\n",
        "    # Append to the list with image name\n",
        "    lbp_features.append([image_name] + lbp_features_list.tolist())\n",
        "\n",
        "# Save features to CSV\n",
        "lbp_df = pd.DataFrame(lbp_features)\n",
        "lbp_df.to_csv(lbp_csv, index=False, header=False)\n",
        "print(f'LBP features saved to {lbp_csv}')\n"
      ],
      "metadata": {
        "id": "DZWFp0oCDPq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Color Features (Color Histogram + Color Moments)\n",
        "combined_color_csv = os.path.join(output_path, 'combined_color_features.csv')\n",
        "\n",
        "# Load the color CSV files\n",
        "color_hist_df = pd.read_csv(color_histogram_csv, header=None)\n",
        "color_moments_df = pd.read_csv(color_moments_csv, header=None)\n",
        "\n",
        "# Merge color features\n",
        "combined_color_df = pd.concat([color_hist_df, color_moments_df.iloc[:, 1:]], axis=1)  # Remove duplicate image names\n",
        "combined_color_df.to_csv(combined_color_csv, index=False, header=False)\n",
        "print(f'Combined Color features saved to {combined_color_csv}')\n",
        "\n",
        "# Combine Texture Features (Wavelet + LBP)\n",
        "combined_texture_csv = os.path.join(output_path, 'combined_texture_features.csv')\n",
        "\n",
        "# Load the texture CSV files\n",
        "wavelet_df = pd.read_csv(wavelet_csv, header=None)\n",
        "lbp_df = pd.read_csv(lbp_csv, header=None)\n",
        "\n",
        "# Merge texture features\n",
        "combined_texture_df = pd.concat([wavelet_df, lbp_df.iloc[:, 1:]], axis=1)  # Remove duplicate image names\n",
        "combined_texture_df.to_csv(combined_texture_csv, index=False, header=False)\n",
        "print(f'Combined Texture features saved to {combined_texture_csv}')\n",
        "\n",
        "# Final Handcrafted Feature Combination (Color + Texture)\n",
        "final_handcrafted_csv = os.path.join(output_path, 'final_handcrafted_features.csv')\n",
        "\n",
        "# Merge Color and Texture features\n",
        "final_handcrafted_df = pd.concat([combined_color_df, combined_texture_df.iloc[:, 1:]], axis=1)  # Remove duplicate image names\n",
        "final_handcrafted_df.to_csv(final_handcrafted_csv, index=False, header=False)\n",
        "print(f'Final Handcrafted features saved to {final_handcrafted_csv}')\n"
      ],
      "metadata": {
        "id": "MmW3kb8DDS8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Paths for dataset and feature output\n",
        "input_path = '/content/drive/MyDrive/wangdataset/Images'  # Update this to the Corel 1K image folder path\n",
        "output_path = '/content/drive/MyDrive/DeepLearning'  # Update this to the output folder path\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# List all image files in the input directory\n",
        "image_files = [f for f in os.listdir(input_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Load a pre-trained EfficientNet-B7 model\n",
        "model = models.efficientnet_b7(pretrained=True)\n",
        "# Remove the final classification layer to get features from the penultimate layer\n",
        "model = nn.Sequential(*list(model.children())[:-1])\n",
        "model.eval()\n",
        "\n",
        "# Transformation to resize images to 600x600 and normalize them (EfficientNet-B7 recommended input size)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((600, 600)),  # EfficientNet B7 recommended input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Function to extract deep features using EfficientNet-B7\n",
        "def extract_deep_features5(image_path, model):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(image)\n",
        "\n",
        "    # Flatten the features to a 1D vector\n",
        "    features = features.view(features.size(0), -1).squeeze().numpy()\n",
        "    return features\n",
        "\n",
        "# Directory to save Deep Features\n",
        "deep_features_csv5 = os.path.join(output_path, 'efficientB7.csv')\n",
        "\n",
        "# Process all images for deep features\n",
        "deep_features5 = []\n",
        "\n",
        "for image_name in image_files:\n",
        "    image_path = os.path.join(input_path, image_name)\n",
        "\n",
        "    # Extract deep features\n",
        "    deep_feature_vector5 = extract_deep_features5(image_path, model)\n",
        "\n",
        "    # Append to the list with image name\n",
        "    deep_features5.append([image_name] + deep_feature_vector5.tolist())\n",
        "\n",
        "# Save deep features to CSV\n",
        "deep_features_df5 = pd.DataFrame(deep_features5)\n",
        "deep_features_df5.to_csv(deep_features_csv5, index=False, header=False)\n",
        "print(f'Deep features saved to {deep_features_csv5}')\n"
      ],
      "metadata": {
        "id": "4ISGS6f8ytul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Paths for dataset and output (update these paths as needed)\n",
        "output_path = '/content/drive/MyDrive/DeepLearning'  # Update this to the output folder path\n",
        "final_handcrafted_csv = os.path.join(output_path, 'final_handcrafted_features.csv')  # Path to handcrafted feature CSV\n",
        "deep_features_csv5 = os.path.join(output_path, 'efficientB7.csv')  # Path for EfficientNet-B7 deep features CSV\n",
        "\n",
        "# Final combined feature CSV (Handcrafted + EfficientNet-B7)\n",
        "final_combined_csv = os.path.join(output_path, 'final_combined_efficientB7.csv')\n",
        "\n",
        "# Load the handcrafted and EfficientNet-B7 feature CSV files\n",
        "handcrafted_df = pd.read_csv(final_handcrafted_csv, header=None)\n",
        "deep_features_df5 = pd.read_csv(deep_features_csv5, header=None)\n",
        "\n",
        "# Ensure the same order by merging on image names (first column)\n",
        "combined_df = pd.merge(handcrafted_df, deep_features_df5, on=0)  # Merge using image names (first column)\n",
        "\n",
        "# Save the final combined features to CSV\n",
        "combined_df.to_csv(final_combined_csv, index=False, header=False)\n",
        "print(f'Final combined features (Handcrafted + EfficientNet-B7) saved to {final_combined_csv}')\n"
      ],
      "metadata": {
        "id": "CjXLOXwm1r5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "csv_file_path = '/content/drive/MyDrive/DeepLearning/efficientB7.csv'  # Path to your combined CSV\n",
        "\n",
        "# Load the CSV file without headers\n",
        "df = pd.read_csv(csv_file_path, header=None)\n",
        "\n",
        "# Determine the number of feature columns (excluding the filename)\n",
        "num_features = df.shape[1] - 1\n",
        "\n",
        "# Create the header with 'filename' and 'feature1', 'feature2', ..., 'featureN'\n",
        "header = ['filename'] + [f'feature{i+1}' for i in range(num_features)]\n",
        "\n",
        "# Insert the header as the first row\n",
        "df.columns = header\n",
        "\n",
        "# Save the updated DataFrame back to CSV with the new header\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Print the first row to verify the headers\n",
        "print(\"Updated CSV Headers:\")\n",
        "print(df.head(1))\n"
      ],
      "metadata": {
        "id": "Vt5zJL_nX0X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import (\n",
        "    euclidean, cosine, canberra, cityblock, chebyshev, minkowski, mahalanobis, hamming, jaccard\n",
        ")\n",
        "from scipy.spatial import distance\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Step 1: Load features\n",
        "features_file = \"/content/drive/MyDrive/DeepLearning/final_combined_efficientB7.csv\"  # Update with your file path\n",
        "data = pd.read_csv(features_file)\n",
        "\n",
        "# Check if there is a column with filenames (adjust if needed)\n",
        "filename_column = \"filename\"  # Replace with the actual name of the filename column\n",
        "if filename_column not in data.columns:\n",
        "    raise ValueError(f\"Column '{filename_column}' not found in the dataset.\")\n",
        "# Extract filenames\n",
        "filenames = data[filename_column].tolist()\n",
        "\n",
        "# Handle 0s by replacing with a small epsilon if necessary\n",
        "data = data.replace(0, np.finfo(float).eps)\n",
        "\n",
        "# Ensure all columns are numeric (drop non-numeric columns)\n",
        "numeric_data = data.select_dtypes(include=[np.number])\n",
        "\n",
        "# Generate labels based on filenames\n",
        "def generate_label_from_filename(filename):\n",
        "    # Extract the numeric part from the filename (assuming filenames are like '0.jpg', '1.jpg', ...)\n",
        "    image_number = int(filename.split('.')[0])\n",
        "    # Determine the class based on the image number\n",
        "    return (image_number // 100) + 1  # Class 1 for 0-99, Class 2 for 100-199, etc.\n",
        "\n",
        "# Generate labels\n",
        "labels = np.array([generate_label_from_filename(fname) for fname in filenames])\n",
        "\n",
        "# Normalize features using Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "normalized_features = scaler.fit_transform(numeric_data.values)\n",
        "\n",
        "# Regularize the covariance matrix if it's singular\n",
        "def compute_inverse_covariance(features):\n",
        "    try:\n",
        "        # Compute the covariance matrix\n",
        "        cov_matrix = np.cov(features.T)\n",
        "        # Add a small regularization to the diagonal (identity matrix)\n",
        "        regularization = 1e-5 * np.eye(cov_matrix.shape[0])\n",
        "        regularized_cov = cov_matrix + regularization\n",
        "        # Attempt to invert the regularized covariance matrix\n",
        "        return np.linalg.inv(regularized_cov)\n",
        "    except np.linalg.LinAlgError:\n",
        "        print(\"Error: Even the regularized covariance matrix is singular.\")\n",
        "        return None\n",
        "\n",
        "# Pre-compute the inverse covariance matrix for Mahalanobis distance\n",
        "VI = compute_inverse_covariance(normalized_features)\n",
        "\n",
        "# Define similarity metrics including normalized Euclidean\n",
        "def calculate_similarity(metric, query_feature, all_features):\n",
        "    try:\n",
        "        if metric == \"normalized_euclidean\":\n",
        "            return np.array([euclidean(query_feature, f) for f in all_features])  # Using normalized features\n",
        "        elif metric == \"cosine\":\n",
        "            return np.array([cosine(query_feature, f) for f in all_features])\n",
        "        elif metric == \"canberra\":\n",
        "            return np.array([canberra(query_feature, f) for f in all_features])\n",
        "        elif metric == \"manhattan\":\n",
        "            return np.array([cityblock(query_feature, f) for f in all_features])\n",
        "        elif metric == \"chebyshev\":\n",
        "            return np.array([chebyshev(query_feature, f) for f in all_features])\n",
        "        elif metric == \"minkowski\":\n",
        "            return np.array([minkowski(query_feature, f, 3) for f in all_features])  # p=3 for Minkowski\n",
        "        elif metric == \"mahalanobis\" and VI is not None:\n",
        "            return np.array([mahalanobis(query_feature, f, VI) for f in all_features])\n",
        "        elif metric == \"hamming\":\n",
        "            return np.array([hamming(query_feature, f) for f in all_features])\n",
        "        elif metric == \"jaccard\":\n",
        "            return np.array([jaccard(query_feature > 0, f > 0) for f in all_features])  # Convert to binary for Jaccard\n",
        "        else:\n",
        "            raise ValueError(\"Unknown metric or Mahalanobis distance not available\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in calculating {metric} distance: {e}\")\n",
        "        return np.full(len(all_features), np.inf)  # Return infinite distances on failure\n",
        "\n",
        "# List of similarity metrics to evaluate, including normalized Euclidean\n",
        "similarity_metrics = [\n",
        "    \"normalized_euclidean\", \"cosine\", \"canberra\", \"manhattan\", \"chebyshev\",\n",
        "    \"minkowski\", \"mahalanobis\", \"hamming\", \"jaccard\"\n",
        "]\n",
        "\n",
        "# Initialize storage for results\n",
        "results = {metric: {\n",
        "    \"precision_10\": [], \"recall_10\": [], \"accuracy_10\": [], \"f1_10\": [],\n",
        "    \"precision_20\": [], \"recall_20\": [], \"accuracy_20\": [], \"f1_20\": [],\n",
        "    \"precision_30\": [], \"recall_30\": [], \"accuracy_30\": [], \"f1_30\": [],\n",
        "    \"precision_40\": [], \"recall_40\": [], \"accuracy_40\": [], \"f1_40\": [],\n",
        "    \"precision_50\": [], \"recall_50\": [], \"accuracy_50\": [], \"f1_50\": []\n",
        "} for metric in similarity_metrics}\n",
        "total_images = len(normalized_features)\n",
        "\n",
        "# Step 2: Evaluate for each image for top 10, 20, 30, 40, and 50 images\n",
        "for idx in range(total_images):\n",
        "    query_feature = normalized_features[idx]\n",
        "    query_label = labels[idx]\n",
        "\n",
        "    for metric in similarity_metrics:\n",
        "        similarities = calculate_similarity(metric, query_feature, normalized_features)\n",
        "\n",
        "        # Check for invalid similarity values\n",
        "        if np.any(np.isnan(similarities)) or np.any(np.isinf(similarities)):\n",
        "            print(f\"Warning: Invalid values detected in {metric} similarities for image index {idx}\")\n",
        "            continue\n",
        "\n",
        "        sorted_indices = np.argsort(similarities)\n",
        "        predictions = labels[sorted_indices]  # Sorted labels based on similarity\n",
        "\n",
        "        for top_k in [10, 20, 30, 40, 50]:\n",
        "            # Total number of relevant images retrieved in the top_k\n",
        "            relevant_retrieved = np.sum(predictions[:top_k] == query_label)\n",
        "\n",
        "            # Calculate Precision, Recall, Accuracy, and F1 Score\n",
        "            precision = relevant_retrieved / top_k  # Total images retrieved is top_k\n",
        "            recall = relevant_retrieved / 100       # Total images in the class is 100\n",
        "            accuracy = relevant_retrieved / top_k   # Accuracy is same as precision in this context\n",
        "            f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            # Store results\n",
        "            results[metric][f\"precision_{top_k}\"].append(precision)\n",
        "            results[metric][f\"recall_{top_k}\"].append(recall)\n",
        "            results[metric][f\"accuracy_{top_k}\"].append(accuracy)\n",
        "            results[metric][f\"f1_{top_k}\"].append(f1_score)\n",
        "\n",
        "# Step 3: Calculate average metrics for each similarity measure without dictionary comprehension\n",
        "average_results = {}\n",
        "\n",
        "for metric in similarity_metrics:\n",
        "    average_results[metric] = {\n",
        "        \"precision_10\": np.mean(results[metric][\"precision_10\"]),\n",
        "        \"recall_10\": np.mean(results[metric][\"recall_10\"]),\n",
        "        \"accuracy_10\": np.mean(results[metric][\"accuracy_10\"]),\n",
        "        \"f1_10\": np.mean(results[metric][\"f1_10\"]),\n",
        "        \"precision_20\": np.mean(results[metric][\"precision_20\"]),\n",
        "        \"recall_20\": np.mean(results[metric][\"recall_20\"]),\n",
        "        \"accuracy_20\": np.mean(results[metric][\"accuracy_20\"]),\n",
        "        \"f1_20\": np.mean(results[metric][\"f1_20\"]),\n",
        "        \"precision_30\": np.mean(results[metric][\"precision_30\"]),\n",
        "        \"recall_30\": np.mean(results[metric][\"recall_30\"]),\n",
        "        \"accuracy_30\": np.mean(results[metric][\"accuracy_30\"]),\n",
        "        \"f1_30\": np.mean(results[metric][\"f1_30\"]),\n",
        "        \"precision_40\": np.mean(results[metric][\"precision_40\"]),\n",
        "        \"recall_40\": np.mean(results[metric][\"recall_40\"]),\n",
        "        \"accuracy_40\": np.mean(results[metric][\"accuracy_40\"]),\n",
        "        \"f1_40\": np.mean(results[metric][\"f1_40\"]),\n",
        "        \"precision_50\": np.mean(results[metric][\"precision_50\"]),\n",
        "        \"recall_50\": np.mean(results[metric][\"recall_50\"]),\n",
        "        \"accuracy_50\": np.mean(results[metric][\"accuracy_50\"]),\n",
        "        \"f1_50\": np.mean(results[metric][\"f1_50\"])\n",
        "    }\n",
        "\n",
        "# Step 4: Print average results for top 10, 20, 30, 40, and 50\n",
        "for metric, metrics_results in average_results.items():\n",
        "    print(f\"Metric: {metric}\")\n",
        "    for top_k in [10, 20, 30, 40, 50]:\n",
        "        print(f\"  Average Precision (Top {top_k}): {metrics_results[f'precision_{top_k}']:.4f}\")\n",
        "        print(f\"  Average Recall (Top {top_k}): {metrics_results[f'recall_{top_k}']:.4f}\")\n",
        "        print(f\"  Average Accuracy (Top {top_k}): {metrics_results[f'accuracy_{top_k}']:.4f}\")\n",
        "        print(f\"  Average F1 Score (Top {top_k}): {metrics_results[f'f1_{top_k}']:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "HTcIL9Cn4xMa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}